{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "tf_image_generator.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kfZHozQazKkh"
      },
      "outputs": [],
      "source": [
        "!wget https://storage.googleapis.com/tensorflow-1-public/course2/week3/horse-or-human.zip\n",
        "!wget https://storage.googleapis.com/tensorflow-1-public/course2/week3/validation-horse-or-human.zip"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import zipfile\n",
        "\n",
        "# Unzip the dataset\n",
        "local_zip = './horse-or-human.zip'\n",
        "zip_ref = zipfile.ZipFile(local_zip, 'r')\n",
        "zip_ref.extractall('./horse-or-human')\n",
        "\n",
        "local_zip = './validation-horse-or-human.zip'\n",
        "zip_ref = zipfile.ZipFile(local_zip, 'r')\n",
        "zip_ref.extractall('./validation-horse-or-human')\n",
        "\n",
        "zip_ref.close()"
      ],
      "metadata": {
        "id": "AD5dGApCzQk1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.image as mpimg\n",
        "\n",
        "train_horse_dir = os.path.join('./horse-or-human/horses')\n",
        "train_human_dir = os.path.join('./horse-or-human/humans')\n",
        "validation_horse_dir = os.path.join('./validation-horse-or-human/horses')\n",
        "validation_human_dir = os.path.join('./validation-horse-or-human/humans')\n",
        "\n",
        "#print file names\n",
        "train_horse_names = os.listdir(train_horse_dir)\n",
        "print(train_horse_names[:10])\n",
        "print(f'number of horse label: {len(train_horse_names)}\\n')\n",
        "\n",
        "train_human_names = os.listdir(train_human_dir)\n",
        "print(train_human_names[:10])\n",
        "print(f'number of human label: {len(train_human_names)}\\n')\n",
        "\n",
        "# Directory with validation horse pictures\n",
        "val_horse_names = os.listdir(validation_horse_dir)\n",
        "print(val_horse_names[:10])\n",
        "print(f'number of horse label: {len(val_horse_names)}\\n')\n",
        "\n",
        "# Directory with validation human pictures\n",
        "val_human_names = os.listdir(validation_human_dir)\n",
        "print(val_human_names[:10])\n",
        "print(f'number of horse label: {len(val_human_names)}\\n')\n",
        "\n",
        "#plot the some exaple images\n",
        "nrows=4 \n",
        "ncols=4\n",
        "pic_index = 0\n",
        "fig = plt.gcf()\n",
        "fig.set_size_inches(ncols * 4, nrows * 4)\n",
        "\n",
        "pic_index += 8\n",
        "next_horse_pix = [os.path.join(train_horse_dir, fname) \n",
        "                for fname in train_horse_names[pic_index-8:pic_index]]\n",
        "next_human_pix = [os.path.join(train_human_dir, fname) \n",
        "                for fname in train_human_names[pic_index-8:pic_index]]\n",
        "\n",
        "for i, img_path in enumerate(next_horse_pix+next_human_pix):\n",
        "  # Set up subplot; subplot indices start at 1\n",
        "  sp = plt.subplot(nrows, ncols, i + 1)\n",
        "  sp.axis('Off') # Don't show axes (or gridlines)\n",
        "\n",
        "  img = mpimg.imread(img_path)\n",
        "  plt.imshow(img)\n",
        "\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "fjW8ojJ8zoKx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# build model for classification\n",
        "import tensorflow as tf\n",
        "import tensorflow.keras as keras\n",
        "from tensorflow.keras.optimizers import RMSprop\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "\n",
        "model = keras.models.Sequential([\n",
        "  # This is the first convolution\n",
        "    keras.layers.Conv2D(16, (3,3), activation='relu', input_shape=(300, 300, 3)),\n",
        "    keras.layers.MaxPooling2D(2, 2),\n",
        "    # The second convolution\n",
        "    keras.layers.Conv2D(32, (3,3), activation='relu'),\n",
        "    keras.layers.MaxPooling2D(2,2),\n",
        "    # The third convolution\n",
        "    keras.layers.Conv2D(64, (3,3), activation='relu'),\n",
        "    keras.layers.MaxPooling2D(2,2),\n",
        "    # The fourth convolution\n",
        "    keras.layers.Conv2D(64, (3,3), activation='relu'),\n",
        "    keras.layers.MaxPooling2D(2,2),\n",
        "    # The fifth convolution\n",
        "    keras.layers.Conv2D(64, (3,3), activation='relu'),\n",
        "    keras.layers.MaxPooling2D(2,2),\n",
        "    # Flatten the results to feed into a DNN\n",
        "    keras.layers.Flatten(),\n",
        "    # 512 neuron hidden layer\n",
        "    keras.layers.Dense(512, activation='relu'),\n",
        "    # Only 1 output neuron. It will contain a value from 0-1 where 0 for 1 class ('horses') and 1 for the other ('humans')\n",
        "    keras.layers.Dense(1, activation='sigmoid')\n",
        "])\n",
        "\n",
        "model.summary()\n",
        "\n",
        "model.compile(loss='binary_crossentropy',\n",
        "              optimizer=RMSprop(learning_rate=0.001),\n",
        "              metrics=['accuracy'])\n",
        "\n",
        "train_data_gen = ImageDataGenerator(rescale=1/255)\n",
        "val_data_gen = ImageDataGenerator(rescale=1/255)\n",
        "\n",
        "train_generator = train_data_gen.flow_from_directory(\n",
        "    './horse-or-human/',\n",
        "    target_size=(300, 300),\n",
        "    batch_size=128,\n",
        "    class_mode='binary'\n",
        ")\n",
        "\n",
        "val_generator = train_data_gen.flow_from_directory(\n",
        "    './validation-horse-or-human/',\n",
        "    target_size=(300, 300),\n",
        "    batch_size=32,\n",
        "    class_mode='binary'\n",
        ")"
      ],
      "metadata": {
        "id": "lT_S-MCH113M"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "history = model.fit(train_generator, steps_per_epoch=8, epochs=15, verbose=1, validation_data=val_generator, validation_steps=8)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EtsmMBH39UVu",
        "outputId": "3173e207-a854-4778-a9f5-21985b38aa9c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/15\n",
            "8/8 [==============================] - 94s 11s/step - loss: 0.6552 - accuracy: 0.6552 - val_loss: 0.5370 - val_accuracy: 0.8555\n",
            "Epoch 2/15\n",
            "8/8 [==============================] - 88s 11s/step - loss: 0.4920 - accuracy: 0.7786 - val_loss: 0.8130 - val_accuracy: 0.7852\n",
            "Epoch 3/15\n",
            "8/8 [==============================] - 87s 11s/step - loss: 0.5244 - accuracy: 0.8454 - val_loss: 1.3574 - val_accuracy: 0.7344\n",
            "Epoch 4/15\n",
            "8/8 [==============================] - 87s 12s/step - loss: 0.2874 - accuracy: 0.8954 - val_loss: 0.7291 - val_accuracy: 0.8281\n",
            "Epoch 5/15\n",
            "8/8 [==============================] - 88s 11s/step - loss: 1.0662 - accuracy: 0.8665 - val_loss: 0.4981 - val_accuracy: 0.7930\n",
            "Epoch 6/15\n",
            "8/8 [==============================] - 87s 12s/step - loss: 0.2354 - accuracy: 0.9121 - val_loss: 1.2344 - val_accuracy: 0.8047\n",
            "Epoch 7/15\n",
            "8/8 [==============================] - 98s 12s/step - loss: 0.1171 - accuracy: 0.9629 - val_loss: 1.5042 - val_accuracy: 0.8008\n",
            "Epoch 8/15\n",
            "8/8 [==============================] - 87s 11s/step - loss: 0.1771 - accuracy: 0.9333 - val_loss: 0.6983 - val_accuracy: 0.8633\n",
            "Epoch 9/15\n",
            "8/8 [==============================] - 87s 12s/step - loss: 0.0733 - accuracy: 0.9744 - val_loss: 0.6307 - val_accuracy: 0.8672\n",
            "Epoch 10/15\n",
            "8/8 [==============================] - 88s 11s/step - loss: 0.0431 - accuracy: 0.9833 - val_loss: 1.4525 - val_accuracy: 0.8281\n",
            "Epoch 11/15\n",
            "8/8 [==============================] - 87s 12s/step - loss: 0.2313 - accuracy: 0.9388 - val_loss: 1.1581 - val_accuracy: 0.8281\n",
            "Epoch 12/15\n",
            "8/8 [==============================] - 87s 11s/step - loss: 0.0894 - accuracy: 0.9633 - val_loss: 1.4400 - val_accuracy: 0.8203\n",
            "Epoch 13/15\n",
            "8/8 [==============================] - 87s 11s/step - loss: 0.1457 - accuracy: 0.9344 - val_loss: 1.0335 - val_accuracy: 0.8516\n",
            "Epoch 14/15\n",
            "8/8 [==============================] - 97s 12s/step - loss: 0.0512 - accuracy: 0.9805 - val_loss: 1.9371 - val_accuracy: 0.8047\n",
            "Epoch 15/15\n",
            "8/8 [==============================] - 87s 11s/step - loss: 0.0057 - accuracy: 1.0000 - val_loss: 2.4045 - val_accuracy: 0.8047\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#model prediction\n",
        "import os\n",
        "import numpy as np\n",
        "from google.colab import files\n",
        "from keras.preprocessing import image\n",
        "\n",
        "uploaded = files.upload()\n",
        "\n",
        "for fn in uploaded.keys():\n",
        " \n",
        "  # predicting images\n",
        "  path = '/content/' + fn\n",
        "  img = image.load_img(path, target_size=(300, 300))\n",
        "  x = image.img_to_array(img)\n",
        "  x /= 255\n",
        "  x = np.expand_dims(x, axis=0)\n",
        "\n",
        "  images = np.vstack([x])\n",
        "  classes = model.predict(images, batch_size=10)\n",
        "  print(classes[0])\n",
        "    \n",
        "  if classes[0]>0.5:\n",
        "    print(fn + \" is a human\")\n",
        "  else:\n",
        "    print(fn + \" is a horse\")\n",
        "\n",
        "# # CODE BLOCK FOR SAFARI USERS\n",
        "\n",
        "images = os.listdir(\"/tmp/images\")\n",
        "\n",
        "print(images)\n",
        "\n",
        "for i in images:\n",
        "  print()\n",
        "  # predicting images\n",
        "  path = '/tmp/images/' + i\n",
        "  img = image.load_img(path, target_size=(300, 300))\n",
        "  x = image.img_to_array(img)\n",
        "  x /= 255\n",
        "  x = np.expand_dims(x, axis=0)\n",
        "\n",
        "  images = np.vstack([x])\n",
        "  classes = model.predict(images, batch_size=10)\n",
        "  print(classes[0])\n",
        "  if classes[0]>0.5:\n",
        "    print(i + \" is a human\")\n",
        "  else:\n",
        "    print(i + \" is a horse\")\n",
        "\n",
        " "
      ],
      "metadata": {
        "id": "cD2VAF6L9jOr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import random\n",
        "from tensorflow.keras.preprocessing.image import img_to_array, load_img\n",
        "\n",
        "# Define a new Model that will take an image as input, and will output\n",
        "# intermediate representations for all layers in the previous model after\n",
        "# the first.\n",
        "successive_outputs = [layer.output for layer in model.layers[1:]]\n",
        "visualization_model = tf.keras.models.Model(inputs = model.input, outputs = successive_outputs)\n",
        "\n",
        "# Prepare a random input image from the training set.\n",
        "horse_img_files = [os.path.join(train_horse_dir, f) for f in train_horse_names]\n",
        "human_img_files = [os.path.join(train_human_dir, f) for f in train_human_names]\n",
        "img_path = random.choice(horse_img_files + human_img_files)\n",
        "\n",
        "img = load_img(img_path, target_size=(300, 300))  # this is a PIL image\n",
        "x = img_to_array(img)  # Numpy array with shape (300, 300, 3)\n",
        "x = x.reshape((1,) + x.shape)  # Numpy array with shape (1, 300, 300, 3)\n",
        "\n",
        "# Scale by 1/255\n",
        "x /= 255\n",
        "\n",
        "# Run the image through the network, thus obtaining all\n",
        "# intermediate representations for this image.\n",
        "successive_feature_maps = visualization_model.predict(x)\n",
        "\n",
        "# These are the names of the layers, so you can have them as part of the plot\n",
        "layer_names = [layer.name for layer in model.layers[1:]]\n",
        "\n",
        "# Display the representations\n",
        "for layer_name, feature_map in zip(layer_names, successive_feature_maps):\n",
        "  if len(feature_map.shape) == 4:\n",
        "\n",
        "    # Just do this for the conv / maxpool layers, not the fully-connected layers\n",
        "    n_features = feature_map.shape[-1]  # number of features in feature map\n",
        "\n",
        "     # The feature map has shape (1, size, size, n_features)\n",
        "    size = feature_map.shape[1]\n",
        "    \n",
        "    # Tile the images in this matrix\n",
        "    display_grid = np.zeros((size, size * n_features))\n",
        "    for i in range(n_features):\n",
        "      x = feature_map[0, :, :, i]\n",
        "      x -= x.mean()\n",
        "      x /= x.std()\n",
        "      x *= 64\n",
        "      x += 128\n",
        "      x = np.clip(x, 0, 255).astype('uint8')\n",
        "    \n",
        "      # Tile each filter into this big horizontal grid\n",
        "      display_grid[:, i * size : (i + 1) * size] = x\n",
        "    \n",
        "    # Display the grid\n",
        "    scale = 20. / n_features\n",
        "    plt.figure(figsize=(scale * n_features, scale))\n",
        "    plt.title(layer_name)\n",
        "    plt.grid(False)\n",
        "    plt.imshow(display_grid, aspect='auto', cmap='viridis')\n"
      ],
      "metadata": {
        "id": "YkrOfNcEGlSc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os, signal\n",
        "os.kill(os.getpid(), signal.SIGKILL)"
      ],
      "metadata": {
        "id": "qi4P1xVaG5DJ"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}